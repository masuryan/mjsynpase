# This notebook will read data from ADLS Gen2 (folder used by Azure Databricks as mount) and write the clensed data to One Lake (DataLake called Bronze)

# Set the ADLS Gen2 account name and file system name
account_name = "bigdatademostorage"
file_system = "demodatasets" # this is your Blob Container name

# Set the OAuth2 credentials
client_id = "XXXX"
client_secret = "XXXX"
tenant_id = "XXXX"

# Set the configs for Spark session
spark.conf.set(f"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net", "OAuth")
spark.conf.set(f"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set(f"fs.azure.account.oauth2.client.id.{account_name}.dfs.core.windows.net", client_id)
spark.conf.set(f"fs.azure.account.oauth2.client.secret.{account_name}.dfs.core.windows.net", client_secret)
spark.conf.set(f"fs.azure.account.oauth2.client.endpoint.{account_name}.dfs.core.windows.net", f"https://login.microsoftonline.com/{tenant_id}/oauth2/token")

# Read the file from ADLS Gen2 (outside OneLake and this location is used by Azure Databricks as mount) into a dataframe
file_path = f"abfss://{file_system}@{account_name}.dfs.core.windows.net/tpch/raw/tpchdata"
df = spark.read.format("delta").load(file_path)
display(df)

df.write.format("delta").mode("overwrite").saveAsTable("tpchdatatable")
# this will write to abfss://XXXX@onelake.dfs.fabric.microsoft.com/Bronze.Lakehouse/Tables/tpchdatatable on your onelake (i have created a lakehouse called Bronze)